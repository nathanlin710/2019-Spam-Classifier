{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, re\n",
    "\n",
    "# there are total of 65 xlsx files in data folder\n",
    "# invalid tweet text stored in sheet 1, sheet_name='5-2-2017.tweets'\n",
    "# valid tweet text stored in sheet 2, sheet_name='5-2-2017.tweets.TRUE'\n",
    "dfs_false = []\n",
    "dfs_true = []\n",
    "for fname in os.listdir(\"/Users/sandypli/Desktop/Calls/2019SpamClassify/data\"):\n",
    "    if re.search(r'\\.xlsx$', fname):\n",
    "        print(fname)\n",
    "        file_name = 'data/' + fname\n",
    "        xls = pd.ExcelFile(file_name)\n",
    "        dfs_false.append(pd.read_excel(file_name, sheet_name=first_sheet_name))\n",
    "        #dfs_true.append(pd.read_excel(file_name, sheet_name=second_sheet_name))\n",
    "        #for df_true in dfs_true:\n",
    "        #    if any(x==0 for x in df_true['T/F'].fillna(1.0).astype(int)):\n",
    "        #        print(fname)\n",
    "\n",
    "#for df_false in dfs_false:\n",
    "    # check to make sure all the value in T/F column is 0\n",
    " #   if any(x==1 for x in df_false['T/F']):\n",
    " #       print(df_false[['T/F']])\n",
    "#for df_true in dfs_true:\n",
    "    # check to make sure all the value in T/F column is 1\n",
    " #   if any(x==0 for x in df_true['T/F'].fillna(1.0).astype(int)):\n",
    "  #      print(df_true.head())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10-27-2017.tweets', '10-27-2017.tweets.TRUE', 'RT']\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "listings = []\n",
    "csv_arr = []\n",
    "file_name = 'data/10-27-2017.tweets.xlsx'\n",
    "xls = pd.ExcelFile(file_name)\n",
    "print(xls.sheet_names)\n",
    "for sheetname in [xls.sheet_names[0], xls.sheet_names[1]]:\n",
    "    listing = pd.read_excel(file_name, sheet_name=sheetname)\n",
    "    listings.append(listing)\n",
    "\n",
    "for listing in listings:\n",
    "    # print(list(listing))\n",
    "    # get \"T/F\" location\n",
    "    answer_index = (list(listing)).index(\"T/F\")\n",
    "    # get \"Tweet Text\" location\n",
    "    text_index = (list(listing)).index(\"Tweet Text\")\n",
    "    for index, row in listing.iterrows():\n",
    "        # skip 1st row as it is headers\n",
    "        if index == 0:\n",
    "            continue\n",
    "\n",
    "        answer = row[answer_index]\n",
    "        text = row[text_index]\n",
    "\n",
    "        if answer == 0:\n",
    "            answer = 'spam'\n",
    "        else:\n",
    "            answer = 'ham'\n",
    "        csv_arr.append([answer, text])\n",
    "print(len(csv_arr))\n",
    "\n",
    "# write csv\n",
    "with open('test.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['v1', 'v2'])\n",
    "\n",
    "    for row in csv_arr:\n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8-25-2017.tweets.xlsx\n",
      "7-4-2017.tweets.xlsx\n",
      "12-1-2017.tweets.xlsx\n",
      "9-25-2017.tweets.xlsx\n",
      "11-15-2017.tweets.xlsx\n",
      "10-20-2017.tweets.xlsx\n",
      "7-20-2017.tweets.xlsx\n",
      "11-20-2017.tweets.xlsx\n",
      "5-11-2017.tweets.xlsx\n",
      "9-14-2017.tweets.xlsx\n",
      "7-24-2017.tweets.xlsx\n",
      "8-8-2017.tweets.xlsx\n",
      "8-14-2017.tweets.xlsx\n",
      "5-30-2017.tweets.xlsx\n",
      "11-1-2017.tweets.xlsx\n",
      "5-3-2017.tweets.xlsx\n",
      "8-3-2017.tweets.xlsx\n",
      "5-24-2017.tweets.xlsx\n",
      "9-21-2017.tweets.xlsx\n",
      "8-21-2017.tweets.xlsx\n",
      "10-11-2017.tweets.xlsx\n",
      "10-3-2017.tweets.xlsx\n",
      "7-5-2017.tweets.xlsx\n",
      "8-30-2017.tweets.xlsx\n",
      "9-11-2017.tweets.xlsx\n",
      "8-11-2017.tweets.xlsx\n",
      "6-21-2017.tweets.xlsx\n",
      "10-25-2017.tweets.xlsx\n",
      "7-25-2017.tweets.xlsx\n",
      "7-31-2017.tweets.xlsx\n",
      "7-10-2017.tweets.xlsx\n",
      "9-5-2017.tweets.xlsx\n",
      "5-2-2017.tweets.xlsx\n",
      "5-17-2017.tweets.xlsx\n",
      "5-22-2017.tweets.xlsx\n",
      "5-5-2017.tweets.xlsx\n",
      "10-17-2017.tweets.xlsx\n",
      "6-29-2017.tweets.xlsx\n",
      "10-13-2017.tweets.xlsx\n",
      "5-18-2017.tweets.xlsx\n",
      "7-13-2017.tweets.xlsx\n",
      "12-7-2017.tweets.xlsx\n",
      "5-1-2017.tweets.xlsx\n",
      "8-1-2017.tweets.xlsx\n",
      "8-16-2017.tweets.xlsx\n",
      "6-26-2017.tweets.xlsx\n",
      "11-8-2017.tweets.xlsx\n",
      "11-26-2017.tweets.xlsx\n",
      "7-18-2017.tweets.xlsx\n",
      "9-28-2017.tweets.xlsx\n",
      "5-16-2017.tweets.xlsx\n",
      "9-8-2017.tweets.xlsx\n",
      "6-16-2017.tweets.xlsx\n",
      "11-6-2017.tweets.xlsx\n",
      "5-4-2017.tweets.xlsx\n",
      "5-23-2017.tweets.xlsx\n",
      "9-18-2017.tweets.xlsx\n",
      "10-5-2017.tweets.xlsx\n",
      "8-22-2017.tweets.xlsx\n",
      "6-12-2017.tweets.xlsx\n",
      "7-19-2017.tweets.xlsx\n",
      "7-27-2017.tweets.xlsx\n",
      "5-12-2017.tweets.xlsx\n",
      "10-27-2017.tweets.xlsx\n",
      "6-27-2017.tweets.xlsx\n",
      "22492\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, re\n",
    "\n",
    "# there are total of 65 xlsx files in data folder\n",
    "# invalid tweet text stored in sheet 1, sheet_name='5-2-2017.tweets'\n",
    "# valid tweet text stored in sheet 2, sheet_name='5-2-2017.tweets.TRUE'\n",
    "# i'll collect data from both xx.tweets and xx.tweets.TRUE sheets\n",
    "dfs = []\n",
    "for fname in os.listdir(\"/Users/sandypli/Desktop/Calls/2019SpamClassify/data\"):\n",
    "    if re.search(r'\\.xlsx$', fname):\n",
    "        #print(fname)\n",
    "        file_name = 'data/' + fname\n",
    "        xls = pd.ExcelFile(file_name)\n",
    "        for sheetname in [xls.sheet_names[0], xls.sheet_names[1]]:\n",
    "            df = pd.read_excel(file_name, sheet_name=sheetname)\n",
    "            dfs.append(df)\n",
    "\n",
    "# this step is to generate one big csv files with 2 columns: 'T/F' and 'Tweet Text'\n",
    "csv_arr = []\n",
    "for df in dfs:\n",
    "    # get \"T/F\" location\n",
    "    answer_index = (list(df)).index(\"T/F\")\n",
    "    # get \"Tweet Text\" location\n",
    "    text_index = (list(df)).index(\"Tweet Text\")\n",
    "    for index, row in df.iterrows():\n",
    "        # skip 1st row as it is headers\n",
    "        if index == 0:\n",
    "            continue\n",
    "\n",
    "        answer = row[answer_index]\n",
    "        text = row[text_index]\n",
    "\n",
    "        if answer == 0:\n",
    "            answer = 'spam'\n",
    "        else:\n",
    "            answer = 'ham'\n",
    "        csv_arr.append([answer, text])\n",
    "print(len(csv_arr))\n",
    "\n",
    "# write csv\n",
    "with open('onebig.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['v1', 'v2'])\n",
    "\n",
    "    for row in csv_arr:\n",
    "        spamwriter.writerow(row)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\n",
      "ham     9609\n",
      "spam    8391\n",
      "Name: v1, dtype: int64\n",
      "test data:\n",
      "ham     2499\n",
      "spam    1993\n",
      "Name: v1, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-ebf101df56ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mvectorize_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/myenv3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \"\"\"\n\u001b[1;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/myenv3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1058\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/myenv3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    971\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/myenv3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    350\u001b[0m                                                tokenize)\n\u001b[1;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 352\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/myenv3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    144\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.calibration import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.svm import *\n",
    "import pandas\n",
    "import csv\n",
    "\n",
    "data = pandas.read_csv('onebig.csv', sep=',', header=0)\n",
    "#print(data.columns.tolist())\n",
    "\n",
    "# we have total 22493 rows in onebig.csv\n",
    "# use ~4/5 for training which is 18000\n",
    "train_data = data[:18000] \n",
    "test_data = data[18000:] \n",
    "print('train data:')\n",
    "print((train_data.v1).value_counts())\n",
    "print('test data:')\n",
    "print(((test_data.v1).value_counts()))\n",
    "classifier = OneVsRestClassifier(SVC(kernel='linear'))\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# train\n",
    "vectorize_text = vectorizer.fit_transform(train_data.v2)\n",
    "classifier.fit(vectorize_text, train_data.v1)\n",
    "\n",
    "# score\n",
    "vectorize_text = vectorizer.transform(test_data.v2)\n",
    "score = classifier.score(vectorize_text, test_data.v1)\n",
    "print('score: {}'.format(score))\n",
    "\n",
    "\n",
    "csv_arr = []\n",
    "for index, row in test_data.iterrows():\n",
    "    answer = row[0]\n",
    "    text = row[1]\n",
    "    vectorize_text = vectorizer.transform([text])\n",
    "    predict = classifier.predict(vectorize_text)[0]\n",
    "    if predict == answer:\n",
    "        result = 'right'\n",
    "    else:\n",
    "        result = 'wrong'\n",
    "    csv_arr.append([len(csv_arr), text, answer, predict, result])\n",
    "\n",
    "\n",
    "# write csv\n",
    "with open('test_score.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=';',\n",
    "            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(['#', 'text', 'answer', 'predict', result])\n",
    "\n",
    "    for row in csv_arr:\n",
    "        spamwriter.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
